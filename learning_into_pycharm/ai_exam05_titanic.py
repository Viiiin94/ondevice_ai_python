# -*- coding: utf-8 -*-
"""AI__exam05_titanic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OiNkBsvXDgbMJjfv8teTiDwKnSmOuPQ2
"""

import seaborn as sns
import pandas as pd
import numpy as np

raw_data = sns.load_dataset('titanic')
print(raw_data.head())

raw_data.info()

raw_data.isnull().sum()

# NaN값이 500개가 넘는 column을 지운다
clean_data = raw_data.dropna(axis=1, thresh=500)
print(clean_data.columns)

# fillna => NaN 값을 채우겠다는 의미 여기서는 평균
clean_data['age'].fillna(clean_data['age'].mean(), inplace=True)
print(clean_data.isnull().sum())

clean_data.age[:20]

# 그냥 column 지움 숫자로 안나오고 중복이라 지움
clean_data.drop(['embark_town', 'alive', 'class'], axis=1, inplace=True, errors='ignore')
print(clean_data.columns)

# embarked가 NaN이면 이전 값으로 대체한다는 뜻 forward fill 반대는 bfill backward fill
clean_data['embarked'].fillna(method='ffill', inplace=True)
print(clean_data.isnull().sum())

clean_data.info()

# 성별 컬럼에서 male은 0으로 female은 1로
clean_data['sex'].replace({'male': 0, 'female': 1}, inplace=True)
# clean_data의 성별 인덱싱
print(clean_data['sex'].unique())

# clean_data의 embarked 인덱싱
print(clean_data.embarked.value_counts())

from re import L
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
clean_data['embarked'] = encoder.fit_transform(clean_data['embarked'])
print(clean_data['embarked'].value_counts())

# 알파벳 순서대로 인덱싱 처리

print(clean_data.who.value_counts())

clean_data['who'] = encoder.fit_transform(clean_data['who'])
print(clean_data['who'].value_counts())

# 마찬가지로 알파벳 순서대로 인덱싱 처리

clean_data.info()

# astype은 타입을 변환 adult_male과 alone은 bool에서 int64로 변환
clean_data['adult_male'] = clean_data['adult_male'].astype('int64')
clean_data['alone'] = clean_data['alone'].astype('int64')
clean_data.info()

target = clean_data[['survived']]
print(type(target))
print(target)

traning_data = clean_data.drop(['survived'], axis=1)
traning_data

value_data = traning_data[['age', 'fare']]
print(value_data.head())

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaled_data = scaler.fit_transform(value_data)
print(type(scaled_data))
value_data = pd.DataFrame(scaled_data, columns=value_data.columns)
print(value_data.head())
print(type(value_data))

traning_data.drop(['age', 'fare'], axis=1, inplace=True)
traning_data

# sex가 male이면 0 아니면 1 female이면 1 아니면 0 이런식으로 데이터 처리
onehot_data = pd.get_dummies(traning_data, columns=traning_data.columns)
onehot_data

onehot_data = onehot_data.astype('int')
onehot_data.info()

train_data = pd.concat([onehot_data, value_data], axis=1)
train_data

# 여기까지가 전처리 과정

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(
    train_data, target, test_size=0.2)
print(X_train.shape)
print(X_test.shape)
print(Y_train.shape)
print(Y_test.shape)

sur = clean_data.loc[clean_data.survived == 1]
sur.mean()

from keras.models import Sequential
from keras.layers import Dense, Dropout

print(X_train.shape)

model = Sequential()
model.add(Dense(128, input_dim = X_train.shape[1], activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

# epochs 학습 횟수를 나타냄 계속 학습시키면 숫자를 바꾸거나 계속 실행하면됨
fit_hist = model.fit(X_train, Y_train,
                     epochs=30,
                     batch_size=64,
                     validation_data=(X_test, Y_test))

import matplotlib.pyplot as plt
plt.plot(fit_hist.history['accuracy'])
plt.plot(fit_hist.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

score = model.evaluate(X_test, Y_test)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

